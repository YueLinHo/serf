[ for now, rough notes for a roadmap of features. eventually, this
  should be a full/polished document with the design roadmap ]

-------------------------------------------------------------------------
Ian Holsman <ianh@apache.org>

Here's are some of the things I would find usefull:

* Simple 'GET/Post/Head' function just does it and returns
   the contents in a bucket-brigade

* Async version of above.. (like LWP::Parallel) I just push
   GET requests and it notifies me when there is an event for
   one of them.

* filters used for 'serf' as similiar in syntax/use to HTTPD

* HTTP/1.1 Support

* SSL Support

* Connection Pooling per server (ie we keep a max of n open connections
   to a server and we re-use these on different requests)

* send requests to a server from a list of servers/ports (via round-robin)

-------------------------------------------------------------------------
Greg Stein <gstein@lyra.org>

* brigades for I/O; possibly as the object representing a complete
  request or response

* high-level funcs/utils for assembling brigades, per above

  [ possibly a multi-tier architecture: lowest level is brigade-based
    output and brigade-assembling input, next level is the filter
    stack, next is a higher-level assembling "objects" for a
    request/response (one generates a request brigade, one is built
    from a response brigade) ]

* push *and* pull modes for requests and responses

* thread-safe for sending requests vs receiving responses

  [ urk. if multiplexing responses, that draws us into the whole MPM
    crap; is there a clean/easy way that we can defer to the app for
    dispatching a response to a thread? note that (at least) we don't
    have accept() problems, just worker threads for responses (no, we
    can't have separate processes for responses) ]

* filters are written in push mode; we use Nifty Coding to operate
  them in a pull fashion

* select/poll based model *and* synchronous model

* http headers flow through filter stack

* no f->r or f->c

* packaged filters: chunk, dechunk, zip, unzip

  [ "http in/out" filters will probably not be exposed; it may even be 
    that we don't use filters for this task ]

* Expect-100 handling

* auto-reconnect, SIGPIPE handling

* very high level functions for simple HTTP (DAV) methods

* proxy support, including proxy auth

* Basic, Digest, and client-cert auth

* HTTP request pipelining

* XML response parsing utilities

  [ this is a real tough one to make intuitive. see Neon for some
    ideas (it isn't super intuitive, but it does provide info for
    considerations that need to be made) ]

* export functionality for httpd; suck in functions from httpd where
  serf might need them

-------------------------------------------------------------------------

POOL DESIGN

From Subversion, we have learned a *lot* about how to use pools in a
heavily structured/object-based environment. Apache is a completely
different beast. "allocate a request pool. use it. destroy it."

Well, in a complex app, that just doesn't cut it. Luckily, the
"proper" use of pools can be described in just a few rules:

* objects should not have their own pools. an object is allocated into
  a pool defined by the constructor's caller. the *CALLER* knows the
  lifetime of the object and will manage it via the pool.

* functions should not create/destroy pools for their operation; they
  should use a pool provided by the caller. again, the *CALLER* knows
  more about how the function will be used, how often, how many times,
  etc. thus, it should be in charge of the function's memory usage.
  
  as an example, the caller might know that the app will exit upon the
  function's return. thus, the function would be created extra work if
  it built/destroyed a pool. instead, it should use the passed-in
  pool, which the caller is going to be tossing as part of app-exit
  anyways.

* whenever an unbounded iteration occurs, a subpool should be
  used. the general pattern is:
  
  subpool = apr_create_subpool(pool);
  for (i = 0; i < n; ++i) {
    do_operation(..., subpool);
    
    apr_pool_clear(subpool);
  }
  apr_pool_destroy(subpool);
  
  This pattern prevents the 'pool' from growing unbounded and
  consuming all of memory.

* given all of the above, it is pretty well mandatory to pass a pool
  to *every* function. since objects are not recording pools for
  themselves, and the caller is always supposed to be managing memory,
  then each function needs a pool, rather than relying on some hidden
  magic pool. in limited cases, objects may record the pool used for
  their construction so that they can construct sub-parts, but these
  cases should be examined carefully.

-------------------------------------------------------------------------
